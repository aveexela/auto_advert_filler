{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4d90c3-bd9b-451c-ace5-0e0d978ed518",
   "metadata": {},
   "source": [
    "# Автоматический парсинг объявлений Avito\n",
    "\n",
    "## Цель\n",
    "\n",
    "Построить универсальный пайплайн для сбора и структурирования объявлений Avito на основе HTML-страниц.  \n",
    "Результат — датасет, пригодный для задач машинного обучения (текст + изображения).\n",
    "\n",
    "---\n",
    "\n",
    "## Пайплайн\n",
    "\n",
    "1. Ручное сохранение HTML-страниц поисковой выдачи Avito  \n",
    "2. Извлечение ссылок на объявления  \n",
    "3. Загрузка HTML объявлений через headless-браузер  \n",
    "4. Парсинг HTML объявлений  \n",
    "5. Извлечение и сохранение изображений  \n",
    "\n",
    "---\n",
    "\n",
    "## Парсинг объявлений\n",
    "\n",
    "Из HTML объявления извлекаются:\n",
    "- заголовок;\n",
    "- описание (сырое и нормализованное);\n",
    "- категории товара;\n",
    "- характеристики;\n",
    "- изображения.\n",
    "\n",
    "Описание извлекается с учётом разных версий верстки Avito (fallback по DOM).  \n",
    "Текст нормализуется и очищается от псевдокириллицы.\n",
    "\n",
    "---\n",
    "\n",
    "## Категории и характеристики\n",
    "\n",
    "Категории извлекаются из breadcrumbs позиционно.  \n",
    "Характеристики извлекаются по DOM-паттерну «название → значение» и автоматически расширяются для разных категорий товаров.\n",
    "\n",
    "---\n",
    "\n",
    "## Изображения\n",
    "\n",
    "Ссылки на изображения извлекаются из клиентского состояния страницы.  \n",
    "Изображения фильтруются по качеству, дедуплицируются и сохраняются локально.\n",
    "\n",
    "---\n",
    "\n",
    "## Результат\n",
    "\n",
    "Формируется структурированный датасет объявлений и набор изображений.  \n",
    "Весь процесс воспроизводим и выполняется офлайн.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693fa80c-6137-48fe-a34e-9fefdeb519a0",
   "metadata": {},
   "source": [
    "**Окружение:**  \n",
    "Python 3.11 (локальный запуск), версии библиотек зафиксированы в `requirements.txt`.\n",
    "\n",
    "## Импорт библиотек и настройка директорий\n",
    "\n",
    "На этом этапе подключаются все необходимые библиотеки: для работы с HTML, изображениями, веб-браузером и файловой системой.  \n",
    "Также определяются директории, в которых будут храниться HTML-страницы объявлений, изображения и ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b2feb83-7d8d-4c40-99bc-e203a9a2f2ae",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "from typing import Set\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import imagehash\n",
    "\n",
    "# Проверка окружения \n",
    "import sys\n",
    "print(\"Python:\", sys.version)\n",
    "\n",
    "# --- Папки ---\n",
    "CATEGORY = \"top_woman\"\n",
    "BASE_DIR = Path(\"data\")\n",
    "HTML_DIR = BASE_DIR / \"html\" / CATEGORY / \"ads\"\n",
    "HTML_LIST_DIR = BASE_DIR / \"html\" / CATEGORY / \"list\"\n",
    "IMG_DIR = BASE_DIR / \"images\" / CATEGORY\n",
    "\n",
    "LINKS_DIR = BASE_DIR / \"links\"\n",
    "DATASETS_DIR = BASE_DIR / \"datasets\"\n",
    "LINKS_PATH = BASE_DIR / \"links\" / f\"links_{CATEGORY}.txt\"\n",
    "OUT_JSONL = BASE_DIR / \"datasets\" / f\"dataset_{CATEGORY}.jsonl\"\n",
    "\n",
    "HTML_LIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "HTML_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LINKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b6625-711c-4e24-b15b-ff8cc1235771",
   "metadata": {},
   "source": [
    "## Извлечение ссылок на объявления из HTML списка\n",
    "\n",
    "Данный блок предназначен для извлечения ссылок на отдельные объявления из HTML-страницы со списком товаров (результаты поиска или категории).\n",
    "\n",
    "### Логика работы\n",
    "\n",
    "1. HTML страницы парсится с помощью BeautifulSoup.\n",
    "2. По атрибуту `data-marker=\"item\"` находятся контейнеры отдельных карточек объявлений.\n",
    "3. Внутри каждой карточки извлекается ссылка `<a>` с атрибутом `href`.\n",
    "4. Из ссылки удаляются GET-параметры.\n",
    "5. С помощью регулярного выражения проверяется наличие идентификатора объявления в конце URL.\n",
    "6. Корректные ссылки приводятся к абсолютному виду и добавляются в множество.\n",
    "\n",
    "### Результат\n",
    "\n",
    "Функция возвращает множество уникальных ссылок на объявления, которые далее используются для загрузки HTML-страниц отдельных товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb00ba14-e5bc-4843-9f08-97b3773a4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_ID_PATTERN = re.compile(r\"_(\\d+)$\")\n",
    "\n",
    "def extract_ad_links_from_list_html(html_text: str):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    links = set()\n",
    "\n",
    "    for item in soup.find_all(\"div\", attrs={\"data-marker\": \"item\"}):\n",
    "        a = item.find(\"a\", href=True)\n",
    "        if not a:\n",
    "            continue\n",
    "\n",
    "        href = a[\"href\"].split(\"?\")[0]\n",
    "        if AD_ID_PATTERN.search(href):\n",
    "            links.add(\"https://www.avito.ru\" + href)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997d7e23-1745-4e5c-8e73-b1fe1cc10e67",
   "metadata": {},
   "source": [
    "## Сбор ссылок на объявления из HTML-страниц списков\n",
    "\n",
    "В данном блоке выполняется пакетная обработка HTML-файлов со списками объявлений.\n",
    "\n",
    "HTML-файлы со списками представляют собой **вручную сохранённые страницы поисковой выдачи Avito для выбранной категории товаров**. Такой подход позволяет работать с локальными HTML без обращения к API и без дополнительных сетевых запросов.\n",
    "\n",
    "Для каждой HTML-страницы:\n",
    "- считывается содержимое файла;\n",
    "- извлекаются ссылки на отдельные объявления с помощью функции `extract_ad_links_from_list_html`;\n",
    "- ссылки агрегируются в общее множество для устранения дубликатов.\n",
    "\n",
    "После обработки всех файлов:\n",
    "- ссылки сортируются;\n",
    "- сохраняются в текстовый файл для дальнейшего использования в пайплайне.\n",
    "\n",
    "### Результат\n",
    "\n",
    "Формируется файл со списком уникальных ссылок на объявления выбранной категории, который используется на следующем этапе загрузки HTML отдельных товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6835c179-9d90-41c9-8739-3b039255ca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List pages: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:07<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique ads: 379\n",
      "Sample: ['https://www.avito.ru/agalatovo/odezhda_obuv_aksessuary/naryadnye_bodi_intimissimi_42_7724048899', 'https://www.avito.ru/anapa/odezhda_obuv_aksessuary/novye_futbolki_barbara_ilvisi_italiya_razmer_s_7598965573', 'https://www.avito.ru/arhangelsk/odezhda_obuv_aksessuary/mayka_top_liu_jo_7832820310', 'https://www.avito.ru/balakovo/odezhda_obuv_aksessuary/svitshot_oversayz_novyy_s_birkoy_razmer_42-50_4381479867', 'https://www.avito.ru/balashiha/odezhda_obuv_aksessuary/futbolka_trikotto_i_bryuki_lavin_raznye_razmery_7427330150']\n",
      "Saved to: data\\links_top_woman.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "list_files = sorted(HTML_LIST_DIR.glob(\"*.html\"))\n",
    "print(\"List pages:\", len(list_files))\n",
    "\n",
    "all_links = set()\n",
    "for f in tqdm(list_files):\n",
    "    html_text = f.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    all_links |= extract_ad_links_from_list_html(html_text)\n",
    "\n",
    "all_links = sorted(all_links)\n",
    "\n",
    "print(\"Total unique ads:\", len(all_links))\n",
    "print(\"Sample:\", all_links[:5])\n",
    "\n",
    "LINKS_PATH.write_text(\"\\n\".join(all_links), encoding=\"utf-8\")\n",
    "print(\"Saved to:\", LINKS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e498cc-50df-4a3b-bf6a-20b818f01912",
   "metadata": {},
   "source": [
    "## Настройка headless-браузера для загрузки HTML\n",
    "\n",
    "В данном блоке настраивается headless-браузер Chrome для автоматизированной загрузки HTML-страниц Avito.\n",
    "\n",
    "Используется режим без графического интерфейса (headless), что позволяет:\n",
    "- запускать браузер в средах без GUI;\n",
    "- ускорить загрузку страниц;\n",
    "- снизить потребление ресурсов.\n",
    "\n",
    "### Основные параметры\n",
    "\n",
    "- `--headless=new` — использование нового headless-режима Chrome;\n",
    "- `--disable-gpu` — отключение GPU для стабильной работы;\n",
    "- `--no-sandbox` — необходим в изолированных средах;\n",
    "- `--window-size=1920,1080` — фиксированный размер окна для корректной загрузки элементов страницы.\n",
    "\n",
    "Браузер используется далее для получения и сохранения HTML-страниц объявлений и поисковой выдачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dda5c1f1-8d9d-490f-98a0-817a91368d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка headless-браузера\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless=new\")  # Новый headless-режим\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86140e0-986f-4b8c-aa90-8cbb9ee5647d",
   "metadata": {},
   "source": [
    "## Загрузка и сохранение HTML-страницы Avito\n",
    "\n",
    "Данная функция предназначена для загрузки HTML-страницы Avito с помощью headless-браузера и сохранения её локально.\n",
    "\n",
    "### Логика работы\n",
    "\n",
    "1. Браузер переходит по заданному URL.\n",
    "2. Выполняется пауза для завершения загрузки JavaScript-контента и формирования клиентского состояния страницы.\n",
    "3. Полученный HTML-код страницы сохраняется в файл без дополнительной обработки.\n",
    "\n",
    "Такой подход позволяет зафиксировать HTML-страницу в том виде, в котором она используется браузером, и далее выполнять парсинг локально, без повторных сетевых запросов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ae62ea6-4288-45b3-9088-d29586dc6430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_save_avito_page(url: str, dest_path: Path):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # дать JS прогрузить JSON\n",
    "    html = driver.page_source\n",
    "    dest_path.write_text(html, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bcc69a-663a-4a24-a793-2af5ed3e09be",
   "metadata": {},
   "source": [
    "## Загрузка HTML-страниц отдельных объявлений\n",
    "\n",
    "В данном блоке выполняется загрузка HTML-страниц отдельных объявлений Avito по заранее собранному списку ссылок.\n",
    "\n",
    "### Логика работы\n",
    "\n",
    "1. Из текстового файла загружается список URL объявлений.\n",
    "2. Для каждого URL извлекается идентификатор объявления (`ad_id`).\n",
    "3. Проверяется наличие соответствующего HTML-файла локально.\n",
    "4. Если файл отсутствует:\n",
    "   - страница загружается с помощью headless-браузера;\n",
    "   - HTML сохраняется в файл;\n",
    "   - выполняется пауза между запросами.\n",
    "\n",
    "### Назначение\n",
    "\n",
    "Такой подход:\n",
    "- предотвращает повторную загрузку уже сохранённых страниц, предупреждая бан ip;\n",
    "- снижает нагрузку на сайт;\n",
    "- позволяет далее выполнять парсинг полностью офлайн, работая только с локальными HTML-файлами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f3e4d71-160f-4fb5-8335-dfcf4a6f8fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 379/379 [1:38:36<00:00, 15.61s/it]\n"
     ]
    }
   ],
   "source": [
    "def load_avito_links(path: Path) -> list:\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "avito_urls = load_avito_links(LINKS_PATH)\n",
    "\n",
    "for url in tqdm(avito_urls):\n",
    "    ad_id = url.split('_')[-1].split(\"?\")[0]\n",
    "    html_path = HTML_DIR / f\"{ad_id}.html\"\n",
    "\n",
    "    if not html_path.exists():\n",
    "        fetch_and_save_avito_page(url, html_path)\n",
    "        time.sleep(4)  # пауза"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69499e06-cc4d-43e5-9558-4310bb8ee18d",
   "metadata": {},
   "source": [
    "## Извлечение ссылок на изображения объявления\n",
    "\n",
    "Данный блок предназначен для извлечения URL изображений из HTML-файла объявления.\n",
    "\n",
    "Изображения на Avito не всегда представлены напрямую в DOM.  \n",
    "Поэтому используется клиентское состояние страницы `window.__preloadedState__`, в котором содержатся все метаданные объявления, включая ссылки на изображения.\n",
    "\n",
    "### Логика работы\n",
    "\n",
    "1. HTML-файл парсится с помощью BeautifulSoup.\n",
    "2. Внутри `<script>`-тегов ищется блок с `window.__preloadedState__`.\n",
    "3. Содержимое состояния:\n",
    "   - декодируется из URL-encoded строки;\n",
    "   - преобразуется в JSON.\n",
    "4. JSON обходится рекурсивно, извлекая все строки, содержащие ссылки на изображения Avito (`img.avito.st/image`).\n",
    "\n",
    "### Результат\n",
    "\n",
    "Функция возвращает множество (`set`) URL изображений объявления.  \n",
    "Дальнейшая загрузка, фильтрация по качеству и дедупликация изображений выполняются в отдельных функциях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e9fa3b-eedd-4961-bbe7-14c6442a0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_urls_from_html_file(html_path: Path) -> Set[str]:\n",
    "    soup = BeautifulSoup(html_path.read_text(encoding=\"utf-8\", errors=\"ignore\"), \"html.parser\")\n",
    "    script_text = None\n",
    "\n",
    "    for s in soup.find_all(\"script\"):\n",
    "        if s.string and \"window.__preloadedState__\" in s.string:\n",
    "            script_text = s.string\n",
    "            break\n",
    "\n",
    "    if not script_text:\n",
    "        return set()\n",
    "\n",
    "    m = re.search(r'window\\.__preloadedState__\\s*=\\s*\"(.+?)\";', script_text)\n",
    "    if not m:\n",
    "        return set()\n",
    "\n",
    "    decoded = urllib.parse.unquote(m.group(1))\n",
    "    try:\n",
    "        data = json.loads(decoded)\n",
    "    except:\n",
    "        return set()\n",
    "\n",
    "    urls = set()\n",
    "    def walk(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for v in obj.values(): walk(v)\n",
    "        elif isinstance(obj, list):\n",
    "            for v in obj: walk(v)\n",
    "        elif isinstance(obj, str) and \"img.avito.st/image\" in obj:\n",
    "            urls.add(obj)\n",
    "\n",
    "    walk(data)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fee0be-864e-4cbb-b89c-a22899c4eafa",
   "metadata": {},
   "source": [
    "## Загрузка и отбор изображений объявления\n",
    "\n",
    "Данный блок отвечает за загрузку изображений по URL и отбор качественных, уникальных фотографий объявления.\n",
    "\n",
    "### Загрузка изображений\n",
    "\n",
    "Функция `download_image`:\n",
    "- загружает изображение по URL;\n",
    "- открывает его с помощью PIL;\n",
    "- приводит к формату RGB;\n",
    "- в случае ошибки возвращает `None`, не прерывая пайплайн.\n",
    "\n",
    "Это обеспечивает устойчивую работу при битых ссылках или сетевых сбоях.\n",
    "\n",
    "### Фильтрация и дедупликация изображений\n",
    "\n",
    "Функция `select_high_quality_images` выполняет несколько этапов обработки:\n",
    "\n",
    "1. **Фильтрация по качеству**  \n",
    "   Оставляются только изображения, у которых минимальная сторона не меньше `min_side` пикселей.\n",
    "\n",
    "2. **Поиск дубликатов**  \n",
    "   Для каждого изображения вычисляется perceptual hash (pHash).  \n",
    "   Изображения группируются, если расстояние между хэшами не превышает порог `hash_thresh`.\n",
    "\n",
    "3. **Выбор лучшего изображения из группы**  \n",
    "   В каждой группе дубликатов сохраняется изображение с максимальным разрешением.\n",
    "\n",
    "### Результат\n",
    "\n",
    "Функция возвращает список уникальных изображений высокого качества, которые далее используются в датасете и мультимодальных моделях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ed0531-0ab6-4231-82f9-d953694f4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url):\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        img = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "        return img\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def select_high_quality_images(urls, min_side=400, hash_thresh=5):\n",
    "    imgs = []\n",
    "    for u in urls:\n",
    "        img = download_image(u)\n",
    "        if img and min(img.size) >= min_side:\n",
    "            imgs.append((u, img))\n",
    "\n",
    "    groups = []\n",
    "    for url, img in imgs:\n",
    "        h = imagehash.phash(img)\n",
    "        for g in groups:\n",
    "            if abs(h - g[0][2]) <= hash_thresh:\n",
    "                g.append((url, img, h))\n",
    "                break\n",
    "        else:\n",
    "            groups.append([(url, img, h)])\n",
    "\n",
    "    final = []\n",
    "    for g in groups:\n",
    "        best = max(g, key=lambda x: x[1].size[0] * x[1].size[1])\n",
    "        final.append((best[0], best[1]))\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414437ab-7219-4496-a02b-a78c4f497a84",
   "metadata": {},
   "source": [
    "## Нормализация текста описаний объявлений\n",
    "### Задача\n",
    "\n",
    "Текстовые описания объявлений, выгруженные из HTML Avito, часто содержат так называемую псевдокириллицу: слова на русском языке, в которых отдельные буквы заменены визуально похожими латинскими символами.\n",
    "Пример: СОБCTBEHHОЕ, PАСЦВЕTОK, ПРОИЗBОДCТВО.\n",
    "\n",
    "Такая подмена:\n",
    "* ухудшает качество текстовых признаков,\n",
    "* мешает токенизации,\n",
    "* приводит к разрыву семантически одинаковых слов.\n",
    "\n",
    "При этом в объявлениях могут встречаться настоящие английские слова (бренды, названия моделей), которые нельзя «насильно» переводить в кириллицу.\n",
    "\n",
    "### Принцип решения\n",
    "\n",
    "Используется контекстная коррекция на уровне слов, а не глобальная замена символов:\n",
    "\n",
    "1. Если слово не содержит кириллицы — оно считается англоязычным и не изменяется.\n",
    "2. Если слово не содержит латиницы — оно уже корректно написано и не изменяется.\n",
    "3. Только если в одном слове одновременно присутствуют кириллица и латиница, латинские символы заменяются на визуально соответствующие кириллические аналоги.\n",
    "\n",
    "Таким образом:\n",
    "* исправляется псевдокириллица,\n",
    "* сохраняются английские слова и бренды,\n",
    "* исключаются ложные замены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134afe7c-0ff2-4dfa-9bb9-b954de398b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAT_TO_RU = {\n",
    "    \"a\": \"а\",\n",
    "    \"b\": \"в\",\n",
    "    \"c\": \"с\",\n",
    "    \"e\": \"е\",\n",
    "    \"h\": \"н\",\n",
    "    \"k\": \"к\",\n",
    "    \"m\": \"м\",\n",
    "    \"o\": \"о\",\n",
    "    \"p\": \"р\",\n",
    "    \"t\": \"т\",\n",
    "    \"x\": \"х\",\n",
    "    \"y\": \"у\",\n",
    "}\n",
    "\n",
    "CYRILLIC_RE = re.compile(r\"[а-яё]\", re.IGNORECASE)\n",
    "LATIN_RE = re.compile(r\"[a-z]\", re.IGNORECASE)\n",
    "\n",
    "def fix_mixed_word(word: str) -> str:\n",
    "    # если в слове нет кириллицы — это почти точно английское слово\n",
    "    if not CYRILLIC_RE.search(word):\n",
    "        return word\n",
    "\n",
    "    # если нет латиницы — тоже ничего не делаем\n",
    "    if not LATIN_RE.search(word):\n",
    "        return word\n",
    "\n",
    "    return \"\".join(LAT_TO_RU.get(ch, ch) for ch in word)\n",
    "\n",
    "def fix_pseudo_cyrillic_text(text: str) -> str:\n",
    "    words = text.split()\n",
    "    fixed_words = [fix_mixed_word(w) for w in words]\n",
    "    return \" \".join(fixed_words)\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zа-яё0-9\\s]\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = fix_pseudo_cyrillic_text(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e14ffd-bd25-4826-8619-a76de4d2ca93",
   "metadata": {},
   "source": [
    "## Парсинг одного HTML-объявления Avito\n",
    "\n",
    "Функция `parse_ad_from_html` выполняет разбор одного локально сохранённого HTML-файла объявления Avito и преобразует его в структурированный словарь признаков.\n",
    "\n",
    "### Извлекаемые данные\n",
    "\n",
    "В процессе парсинга извлекаются:\n",
    "- заголовок объявления;\n",
    "- текст описания (сырое и нормализованное);\n",
    "- категории товара (2 уровня);\n",
    "- хлебные крошки (breadcrumbs);\n",
    "- характеристики товара;\n",
    "- ссылки на изображения и их количество.\n",
    "\n",
    "### Извлечение описания\n",
    "\n",
    "HTML-структура описаний на Avito неоднородна, поэтому используется fallback-логика:\n",
    "\n",
    "1. В первую очередь описание извлекается из блока  \n",
    "   `data-marker=\"item-view/item-description\"`.\n",
    "2. Если блок отсутствует — описание извлекается из контейнера  \n",
    "   `#bx_item-description`, при этом игнорируются заголовки интерфейса.\n",
    "\n",
    "После извлечения текст нормализуется:\n",
    "- приводится к нижнему регистру;\n",
    "- очищается от спецсимволов;\n",
    "- исправляется псевдокириллица при сохранении английских слов.\n",
    "\n",
    "Сохраняются две версии:\n",
    "- `description_raw` — очищенный текст;\n",
    "- `description_clean` — нормализованный текст для модели.\n",
    "\n",
    "### Категории и характеристики\n",
    "\n",
    "Категории извлекаются из breadcrumbs позиционно, без словарей и жёстких названий.  \n",
    "Характеристики извлекаются по DOM-паттерну «название → значение» и не предполагают фиксированный набор полей.\n",
    "\n",
    "### Результат\n",
    "\n",
    "Функция возвращает словарь, соответствующий одному объявлению.  \n",
    "Далее выполняется пакетная обработка всех HTML-файлов с сохранением результатов в список `ads_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56d7bd2e-d8de-4f2f-aa3b-8afe36cdc36a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML files: 379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 379/379 [55:50<00:00,  8.84s/it]\n"
     ]
    }
   ],
   "source": [
    "def parse_ad_from_html(html_path: Path) -> dict:\n",
    "    html_text = html_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "\n",
    "    # ---------- TITLE ----------\n",
    "    title = None\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1:\n",
    "        title = h1.get_text(strip=True)\n",
    "\n",
    "    # ---------- DESCRIPTION ----------\n",
    "    description_raw = None\n",
    "    description_clean = None\n",
    "    \n",
    "    def extract_description(soup):\n",
    "        # 1. Новый редизайн \n",
    "        block = soup.find(attrs={\"data-marker\": \"item-view/item-description\"})\n",
    "        if block:\n",
    "            text = block.get_text(\"\\n\", strip=True)\n",
    "            if text:\n",
    "                return text\n",
    "    \n",
    "        # 2. Старый вариант — внутри bx_item-description, но БЕЗ h2\n",
    "        container = soup.find(id=\"bx_item-description\")\n",
    "        if container:\n",
    "            ps = container.find_all(\"p\")\n",
    "            texts = [p.get_text(strip=True) for p in ps if p.get_text(strip=True)]\n",
    "            if texts:\n",
    "                return \"\\n\".join(texts)\n",
    "    \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    description_raw = extract_description(soup)\n",
    "    if description_raw:\n",
    "        description_clean = normalize_text(description_raw)\n",
    "\n",
    "    # ---------- CATEGORIES ----------\n",
    "    bc = soup.find(attrs={\"data-marker\": \"breadcrumbs\"})\n",
    "    breadcrumbs = []\n",
    "    if bc:\n",
    "        breadcrumbs = [a.get_text(strip=True) for a in bc.find_all(\"a\") if a.get_text(strip=True)]\n",
    "\n",
    "    category_level_1 = None\n",
    "    category_level_2 = None\n",
    "    category_level_3 = None\n",
    "    category_level_4 = None\n",
    "    if len(breadcrumbs) >= 3:\n",
    "        core = breadcrumbs[1:]  # убираем \"Главная\"\n",
    "        if len(core) >= 3:\n",
    "            category_level_1 = core[-4]\n",
    "            category_level_2 = core[-3]\n",
    "            category_level_3 = core[-2]\n",
    "            category_level_4 = core[-1]\n",
    "\n",
    "    # ---------- CHARACTERISTICS ----------\n",
    "    characteristics = {}\n",
    "    for li in soup.find_all(\"li\"):\n",
    "        p = li.find(\"p\")\n",
    "        if not p:\n",
    "            continue\n",
    "\n",
    "        spans = p.find_all(\"span\", recursive=False)\n",
    "        if not spans:\n",
    "            continue\n",
    "\n",
    "        label = spans[0].get_text(strip=True)\n",
    "        if not label:\n",
    "            continue\n",
    "\n",
    "        full_text = p.get_text(\" \", strip=True)\n",
    "        value = full_text[len(label):].strip()\n",
    "        value = value.lstrip(\":\").strip()\n",
    "\n",
    "        if value:\n",
    "            characteristics[label.rstrip(\":\")] = value\n",
    "\n",
    "    # ---------- IMAGES ----------\n",
    "    image_urls = extract_image_urls_from_html_file(html_path)\n",
    "    images = select_high_quality_images(image_urls)\n",
    "\n",
    "    return {\n",
    "        \"ad_id\": html_path.stem,\n",
    "        \"title\": title,\n",
    "        \"description_raw\": description_raw,\n",
    "        \"description_clean\": description_clean,\n",
    "        \"category_level_1\": category_level_1,\n",
    "        \"category_level_2\": category_level_2,\n",
    "        \"category_level_3\": category_level_3,\n",
    "        \"category_level_4\": category_level_4,\n",
    "        \"breadcrumbs_raw\": breadcrumbs,\n",
    "        \"characteristics\": characteristics,\n",
    "        \"images_count\": len(images),\n",
    "        \"image_urls\": [u for u, _ in images],\n",
    "    }\n",
    "\n",
    "ads_data = []\n",
    "\n",
    "html_files = sorted(HTML_DIR.glob(\"*.html\"))\n",
    "print(\"HTML files:\", len(html_files))\n",
    "\n",
    "for html_path in tqdm(html_files):\n",
    "    try:\n",
    "        ad = parse_ad_from_html(html_path)\n",
    "        ads_data.append(ad)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {html_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9742deab-3242-4c6b-99bd-925740d61e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\dataset_top_woman.jsonl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ad_id': '1926983478',\n",
       " 'title': 'Джемпер HV Polo',\n",
       " 'description_raw': '— джемпер HV Polo, очень мало использовали, размер M, маломерит, 1450 руб',\n",
       " 'description_clean': 'джемпер hv polo очень мало использовали размер m маломерит 1450 руб',\n",
       " 'category_level_1': '…',\n",
       " 'category_level_2': 'Женская одежда',\n",
       " 'category_level_3': 'Топы и футболки',\n",
       " 'category_level_4': '46 (M)',\n",
       " 'breadcrumbs_raw': ['Главная',\n",
       "  '…',\n",
       "  '…',\n",
       "  'Женская одежда',\n",
       "  'Топы и футболки',\n",
       "  '46 (M)'],\n",
       " 'characteristics': {'Состояние': 'Хорошее',\n",
       "  'Предмет одежды': 'Поло',\n",
       "  'Размер': '46 (M)',\n",
       "  'Материал основной части': 'Акрил',\n",
       "  'Цвет': 'Красный'},\n",
       " 'images_count': 1,\n",
       " 'image_urls': ['https://70.img.avito.st/image/1/1.d6wkjLa520USO1lIRoVYtNov3U-Qr83HnS_ZQZgl0Uc.bUtOwyqWKO22VU3gY0voPwmE2Fr7DDKWvoJHBT9-x-M']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for ad in ads_data:\n",
    "        f.write(json.dumps(ad, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_JSONL)\n",
    "\n",
    "ads_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d8a3d-9d32-4473-9aac-4fc24cb3c716",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Сохранение изображений объявлений\n",
    "\n",
    "В данном блоке выполняется сохранение изображений объявлений на диск на основе ранее сформированного списка `ads_data`.\n",
    "\n",
    "Для каждого объявления:\n",
    "- используется идентификатор объявления (`ad_id`);\n",
    "- создаётся отдельная директория в `IMG_DIR`;\n",
    "- изображения загружаются по URL и сохраняются в формате JPEG.\n",
    "\n",
    "Используются только изображения, прошедшие предварительную фильтрацию по качеству и дедупликацию.  \n",
    "Ошибки загрузки отдельных изображений не прерывают выполнение пайплайна.\n",
    "\n",
    "### Результат\n",
    "\n",
    "Формируется иерархия каталогов вида:\n",
    "\n",
    "IMG_DIR / ad_id / 00.jpg, 01.jpg, ...\n",
    "\n",
    "Данная структура используется далее для формирования мультимодального датасета (текст + изображения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e22424e1-5e2e-4a65-96c0-7932bff45974",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 379/379 [09:25<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "def save_images_from_ads_data(ads_data, img_dir: Path):\n",
    "    for ad in tqdm(ads_data):\n",
    "        ad_id = ad[\"ad_id\"]\n",
    "        urls = ad.get(\"image_urls\", [])\n",
    "\n",
    "        if not urls:\n",
    "            continue\n",
    "\n",
    "        ad_img_dir = img_dir / ad_id\n",
    "        ad_img_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        for i, url in enumerate(urls):\n",
    "            try:\n",
    "                img = download_image(url)\n",
    "                if img:\n",
    "                    img_path = ad_img_dir / f\"{i:02d}.jpg\"\n",
    "                    img.save(img_path, format=\"JPEG\", quality=95)\n",
    "            except Exception as e:\n",
    "                print(f\"Image error in ad {ad_id}: {e}\")\n",
    "\n",
    "save_images_from_ads_data(ads_data, IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f676d6-eeac-47b2-834c-a759fd142363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
